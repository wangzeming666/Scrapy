Scrapy主要包括了以下组件：

    引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)
    调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 
    由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
    下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
    爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
    项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，
    将被发送到项目管道，并经过几个特定的次序处理数据。
    下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
    爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
    调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。

Scrapy运行流程大概如下：

    首先，引擎从调度器中取出一个链接(URL)用于接下来的抓取
    引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)
    然后，爬虫解析Response
    若是解析出实体（Item）,则交给实体管道进行进一步的处理。
    若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取

在抓取之前, 你需要新建一个Scrapy工程. 进入一个你想用来保存代码的目录，然后执行：
$ scrapy startproject tutorial

    scrapy.cfg: 项目配置文件
    tutorial/: 项目python模块, 之后您将在此加入代码
    tutorial/items.py: 项目items文件
    tutorial/pipelines.py: 项目管道文件
    tutorial/settings.py: 项目配置文件
    tutorial/spiders: 放置spider的目录

定义Item

Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。

通过创建scrapy.Item类, 并且定义类型为 scrapy.Field 的类属性来声明一个Item.

我们通过将需要的item模型化，来控制从 dmoz.org 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。
在 tutorial 目录下的 items.py 文件编辑

from scrapy.item import Item, Field


class DmozItem(Item):
    # define the fields for your item here like:
    name = Field()
    description = Field()
    url = Field()
    
编写Spider

Spider 是用户编写的类, 用于从一个域（或域组）中抓取信息, 定义了用于下载的URL的初步列表, 如何跟踪链接，以及如何来解析这些网页的内容用于提取items。

要建立一个 Spider，继承 scrapy.Spider 基类，并确定三个主要的、强制的属性：

    name：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.
    start_urls：包含了Spider在启动时进行爬取的url列表。因此，第一个被获取到的页面将是其中之一。后续的URL则从初始的URL获取到的数据中提取。
    我们可以利用正则表达式定义和过滤需要进行跟进的链接。
    parse()：是spider的一个方法。被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。
    该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。

这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。

在 /tutorial/tutorial/spiders 目录下创建 dmoz_spider.py
import scrapy

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        filename = response.url.split("/")[-2]
        with open(filename, 'wb') as f:
            f.write(response.body)
            
到项目根目录, 然后运行命令:
$ scrapy crawl dmoz

从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 或者 CSS 表达式机制： Scrapy Selectors

为了方便使用 XPaths，Scrapy 提供 Selector 类， 有四种方法 :

    xpath()：返回selectors列表, 每一个selector表示一个xpath参数表达式选择的节点.
    css() : 返回selectors列表, 每一个selector表示CSS参数表达式选择的节点
    extract()：返回一个unicode字符串，该字符串为XPath选择器返回的数据
    re()： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来

可以通过以下方式提取数据
#通过如下命令选择每个在网站中的 <li> 元素:
sel.xpath('//ul/li') 
 
#网站描述:
sel.xpath('//ul/li/text()').extract()
 
#网站标题:
sel.xpath('//ul/li/a/text()').extract()
 
#网站链接:
sel.xpath('//ul/li/a/@href').extract()
 
#如前所述，每个 xpath() 调用返回一个 selectors 列表，所以我们可以结合 xpath() 去挖掘更深的节点。我们将会用到这些特性，所以:
 
for sel in response.xpath('//ul/li')
    title = sel.xpath('a/text()').extract()
    link = sel.xpath('a/@href').extract()
    desc = sel.xpath('text()').extract()
    print title, link, desc
    
    
在原来的代码中修改：
 
import scrapy

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            title = sel.xpath('a/text()').extract()
            link = sel.xpath('a/@href').extract()
            desc = sel.xpath('text()').extract()
            print title, link, desc
            
            
一般来说，Spider将会将爬取到的数据以 Item 对象返回, 最后修改爬虫类，使用 Item 来保存数据，代码如下
from scrapy.spider import Spider
from scrapy.selector import Selector
from tutorial.items import DmozItem 


class DmozSpider(Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/",
    ]

    def parse(self, response):
        sel = Selector(response)
        sites = sel.xpath('//ul[@class="directory-url"]/li')
        items = []

        for site in sites:
            item = DmozItem()
            item['name'] = site.xpath('a/text()').extract()
            item['url'] = site.xpath('a/@href').extract()
            item['description'] = site.xpath('text()').re('-\s[^\n]*\\r')
            items.append(item)
        return items
        
使用Item Pipeline

当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。

每个item pipeline组件(有时称之为ItemPipeline)是实现了简单方法的Python类。
他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。

以下是item pipeline的一些典型应用：

    清理HTML数据
    验证爬取的数据(检查item包含某些字段)
    查重(并丢弃)
    将爬取结果保存，如保存到数据库、XML、JSON等文件中

编写你自己的item pipeline很简单，每个item pipeline组件是一个独立的Python类，同时必须实现以下方法:        

process_item(item, spider)  #每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象，或是抛出 DropItem异常，
                            #丢弃的item将不会被之后的pipeline组件所处理。
                            
#参数:
item: 由 parse 方法返回的 Item 对象(Item对象)
spider: 抓取到这个 Item 对象对应的爬虫对象(Spider对象)

open_spider(spider)  #当spider被开启时，这个方法被调用。

#参数: 
spider : (Spider object) – 被开启的spider
　　
close_spider(spider)  #当spider被关闭时，这个方法被调用，可以再爬虫关闭后进行相应的数据处理。

#参数: 
spider : (Spider object) – 被关闭的spider


过滤
from scrapy.exceptions import DropItem

class TutorialPipeline(object):

    # put all words in lowercase
    words_to_filter = ['politics', 'religion']

    def process_item(self, item, spider):
        for word in self.words_to_filter:
            if word in unicode(item['description']).lower():
                raise DropItem("Contains forbidden word: %s" % word)
        else:
            return item
            
在 settings.py 中设置ITEM_PIPELINES激活item pipeline，其默认为[]
ITEM_PIPELINES = {'tutorial.pipelines.FilterWordsPipeline': 1}

使用下面的命令存储为json文件格式
scrapy crawl dmoz -o items.json



Scarpy优化豆瓣爬虫的抓取
#item
from scrapy.item import Item, Field


class DoubanItem(Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = Field()  #电影名称
    description = Field()  #电影描述
    url = Field()  #抓取的url
    
主程序
#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
一个简单的Python 爬虫, 用于抓取豆瓣电影Top前250的电影的名称描述等

Anthor: Andrew Liu
Version: 0.0.3
Date: 2014-12-17
Language: Python2.7.8
Editor: Sublime Text2
Operate: 具体操作请看README.md介绍
"""

from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from douban.items import DoubanItem
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor

class DoubanSpider(CrawlSpider) :

    name = "douban" 
    allowed_domains = ["movie.douban.com"]
    start_urls = ["http://movie.douban.com/top250"]
    rules = (
        #将所有符合正则表达式的url加入到抓取列表中
        Rule(SgmlLinkExtractor(allow = (r'http://movie\.douban\.com/top250\?start=\d+&filter=&type=',))),
        #将所有符合正则表达式的url请求后下载网页代码, 形成response后调用自定义回调函数
        Rule(SgmlLinkExtractor(allow = (r'http://movie\.douban\.com/subject/\d+', )), callback = 'parse_page', follow = True),
        )

    def parse_page(self, response) :
        sel = Selector(response)
        item = DoubanItem()
        item['name'] = sel.xpath('//h1/span[@property="v:itemreviewed"]/text()').extract()
        item['description'] = sel.xpath('//div/span[@property="v:summary"]/text()').extract()
        item['url'] = response.url
        return item


未来要解决的问题

    头部伪装
    表单提交
    编码转换

豆瓣抓了一会儿, 还没等我兴奋就被禁掉了
